import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import torch
import time
from model_arch import HybridModel
from trainer import CosmicTrainer, CosmicTester
from DatasetManager import DatasetManager

batch_size = 128

def main():
    # åˆå§‹åŒ–é‡å­è®¾å¤‡
    print("à¸…^â€¢ï»Œâ€¢^à¸… å°èŠ·çš„é‡å­åè®®è¿è¡Œä¸­...å–µï½(â„ â„â€¢â„Ï‰â„â€¢â„ â„)")
    # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ The device is {device} which is in use.")
    # å®šä¹‰æ¨¡å‹ä¿å­˜è·¯å¾„
    save_model_path = f'./best_model_{batch_size}.pth'

    # åˆå§‹åŒ–DatasetManagerï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰
    with DatasetManager(
        # å®šä¹‰è·¯å¾„
        root_dir=r'E:\ML_AMC\train_data',  # å®šä¹‰æ•°æ®é›†csvè·¯å¾„
        dataset_dir=r'E:\ML_AMC\Signal_Dataset\signal_dataset_v3.h5',  # å®šä¹‰åˆ¶ä½œæ•°æ®é›†è·¯å¾„
        index_dir=r'./train_val_test_indices.pkl',  # å®šä¹‰æ•°æ®é›†ç´¢å¼•æ–‡ä»¶è·¯å¾„
        batch_size=batch_size
    ) as dataset_manager:
        # è·å–æ•°æ®åŠ è½½å™¨
        train_loader, valid_loader, test_loader = dataset_manager.get_dataloaders()
        # è·å–æ•°æ®é›†å¤§å°
        train_size, valid_size, test_size = dataset_manager.get_dataset_sizes()
        # è¾“å‡ºæ•°æ®é›†å¤§å°
        print(f'Train size: {train_size}')
        print(f'Validation size: {valid_size}')
        print(f'Test size: {test_size}')
        # éªŒè¯æ•°æ®æ‰¹æ¬¡
        sample_batch = next(iter(train_loader))
        print(f"Batch:")
        print(f"dict_keys: {sample_batch.keys()}")
        print(f"IQ: {sample_batch['iq'].shape}")  # [batch, 2, max_len]
        print("IQ:", sample_batch['iq'].min(), sample_batch['iq'].max())
        print("Symbol width:", sample_batch['sym_width'].min(), sample_batch['sym_width'].max())
        print(f"Symbol sequence: {sample_batch['symbol'].shape}")  # [batch, max_seq_len]
        print(f"Actual data length: {sample_batch['data_len'][:5]}")  # æŸ¥çœ‹å‰5ä¸ªæ ·æœ¬çš„å®é™…é•¿åº¦
        mask = torch.arange(sample_batch['iq'].size(2))[None] < sample_batch['data_len'][:, None]
        print(f"The proportion of valid data is: {mask.float().mean():.2%}.")

        # åœ¨è®­ç»ƒå‰æ·»åŠ äº¤å‰éªŒè¯
        train_indices = set(dataset_manager.train_dataset.indices)
        valid_indices = set(dataset_manager.valid_dataset.indices)
        test_indices = set(dataset_manager.test_dataset.indices)
        # å¼ºåˆ¶æ£€æŸ¥ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸èƒ½æœ‰äº¤é›†
        overlap_train_valid = train_indices & valid_indices
        overlap_train_test = train_indices & test_indices
        overlap_valid_test = valid_indices & test_indices
        print(f"The number of overlapping samples between the train_valid_test sets:\n "
              f"{len(overlap_train_valid)},{len(overlap_train_test)},{len(overlap_valid_test)} (must be 0)")
        assert len(overlap_train_valid) == 0, "æ•°æ®åˆ’åˆ†å­˜åœ¨æ³„æ¼: è®­ç»ƒé›†ä¸éªŒè¯é›†æœ‰é‡å¤!"
        assert len(overlap_train_test) == 0, "æ•°æ®åˆ’åˆ†å­˜åœ¨æ³„æ¼: è®­ç»ƒé›†ä¸æµ‹è¯•é›†æœ‰é‡å¤!"
        assert len(overlap_valid_test) == 0, "æ•°æ®åˆ’åˆ†å­˜åœ¨æ³„æ¼: éªŒè¯é›†ä¸æµ‹è¯•é›†æœ‰é‡å¤!"

        # å®ä¾‹åŒ–æ¨¡å‹
        model = HybridModel()
        model.load_state_dict(torch.load('./best_model_128.pth')['model_state'])
        # å°†æ¨¡å‹è½¬ç§»åˆ°GPU
        model = model.to(device)
        # è®¾ç½®é˜¶æ®µè®­ç»ƒå‚æ•°
        stages_config = [
            {
                'stage_name': 'Warmup',
                'optim_config': {
                    'type': 'RAdam',
                    'params': {'lr': 1e-4, 'weight_decay': 1e-4}
                },
                'criter_config': {'type': 'MAPE'},
                'sched_config': {
                    'type': 'OneCycle',
                    'params': {'max_lr': 1e-3, 'total_steps': 100 * len(train_loader)}
                },
                'epochs': 100,
                'early_stop': 10
            },
            {
                'stage_name': 'FineTune',
                'optim_config': {
                    'type': 'SGD',
                    'params': {'lr': 5e-5, 'momentum': 0.9, 'weight_decay': 5e-5}
                },
                'criter_config': {'type': 'MAPE'},
                'sched_config': {
                    'type': 'plateau',
                    'params': {'factor': 0.5, 'patience': 8}
                },
                'epochs': 100,
                'early_stop': 20
            },
            {
                'stage_name': 'Convergence',   # æ”¶æ•›é˜¶æ®µ
                'optim_config': {
                    'type': 'AdamW',
                    'params': {'lr': 5e-6, 'momentum': 0.9, 'weight_decay': 5e-5}
                },
                'criter_config': {'type': 'MAPE'},
                'sched_config': {
                    'type': 'Cosine',
                    'params': {'T_max': 100, 'eta_min': 1e-6}
                },
                'epochs': 100,
                'early_stop': 30
            }
        ]

        # æ¨¡å‹è®­ç»ƒ
        print('Start Training!')
        start_train_time = time.perf_counter()
        trainer = CosmicTrainer(model, device, dataset_manager)
        trainer.launch(train_loader, valid_loader, stages_config, save_model_path)
        end_train_time = time.perf_counter()
        print('Finished Training!')
        print('Training time: %s Seconds' % (end_train_time - start_train_time))

        # æ¨¡å‹æµ‹è¯•
        print('Start Testing!')
        test_sample = next(iter(test_loader))
        print('Batch:')
        print(f"mean: {test_sample['iq'].mean(dim=(0, 2))}")
        print(f"std: {test_sample['iq'].std(dim=(0, 2))}")
        start_test_time = time.perf_counter()
        # åŠ è½½æœ€ä½³æ¨¡å‹
        print("Loading the best model...")
        checkpoint = torch.load(save_model_path, map_location=device, weights_only=False)
        assert checkpoint['model_arch'] == model.__class__.__name__, "Model architecture mismatch!"
        model.load_state_dict(checkpoint['model_state'])
        # æ¢å¤æ ‡å‡†åŒ–å‚æ•°
        dataset_manager.mean = checkpoint['dataset_stats']['mean']
        dataset_manager.std = checkpoint['dataset_stats']['std']
        print(f"Loaded mean: {dataset_manager.mean}, std: {dataset_manager.std}")
        sample = next(iter(test_loader))
        iq = sample['iq']
        print(f"Tested mean: {iq.mean(dim=(0, 2))}, std: {iq.std(dim=(0, 2))}")

        tester = CosmicTester(model, device, dataset_manager)
        tester.quantum_test(test_loader)
        end_test_time = time.perf_counter()
        print('Finished Testing!')
        print('Testing time: %s Seconds' % (end_test_time - start_test_time))

    print("â™¡âƒ›ãƒ¾(à¹‘â› â–¿ â— à¹‘ ) å°èŠ·æ˜¯æœ€æ£’çš„çŒ«å¨˜ï¼å–µï½(â„ â„â€¢â„Ï‰â„â€¢â„ â„)")


if __name__ == '__main__':
    # â™¡âƒ›ãƒ¾(à¹‘â› â–¿ â— à¹‘ ) å°èŠ·æ˜¯æœ€æ£’çš„çŒ«å¨˜ï¼
    # æ³¨ï¼šçº¯å±èµ›åšçŒ«å¨˜å‘ç”µï¼Œä»¥æ­¤çºªå¿µçŒ«å¨˜å°èŠ· By Aria_Luna_007
    main()
