import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from tqdm import tqdm
from typing import Dict, Any
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR
from optim_criter_sched_factory import OptimizerFactory, CriterionFactory, SchedulerFactory
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from DatasetManager import DatasetManager
from sw_scores import evaluate_sw_score, sw_score_i, plot_pred_vs_actual
from MAPE_Loss_func import mape_loss
import torch.amp as amp

import warnings
warnings.filterwarnings("ignore", category=UserWarning,
                        message="1Torch was not compiled with flash attention.")
warnings.filterwarnings("ignore", category=UserWarning,
                        message="The PyTorch API of nested tensors is in prototype stage")

loss_curve_path = './loss_curve.png'


class CosmicTrainer:
    def __init__(self,
                 model: nn.Module,
                 device: torch.device,
                 dataset_manager: DatasetManager,
                 use_amp: bool = True):
        """
        é‡å­çŒ«å¨˜ç‰¹åˆ¶è®­ç»ƒå™¨

        Args:
            model: è¦è®­ç»ƒçš„æ¨¡åž‹
            device: è®­ç»ƒè®¾å¤‡
            dataset_manager: æ•°æ®é›†ç®¡ç†å¼•å…¥
            use_amp: æ˜¯å¦å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        """
        self.model = model.to(device)
        self.device = device
        self.dataset_manager = dataset_manager
        # å¯ç”¨CUDA Graphä¼˜åŒ–
        self.enable_cuda_graph = torch.cuda.is_available()
        # ä¼˜åŒ–CUDAé…ç½®
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        # æ··åˆç²¾åº¦é…ç½®
        if torch.cuda.is_bf16_supported():
            self.amp_dtype = torch.bfloat16
        else:
            self.amp_dtype = torch.float16
        self.scaler = amp.GradScaler(device='cuda',
                                           init_scale=2**16,
                                           growth_factor=1.5,
                                           backoff_factor=0.5,
                                           enabled=use_amp)
        self.history = {
            'stages': [],
            'train_loss': [],
            'val_loss': [],
            'sw_score': []
        }
        self.static_input = None
        self.static_graph = None

    def configure_stage(self,
                        stage_name: str,
                        optim_config: Dict[str, Any],
                        criter_config: Dict[str, Any],
                        sched_config: Dict[str, Any],
                        epochs: int,
                        early_stop: int
                        ):
        """é…ç½®è®­ç»ƒé˜¶æ®µ"""
        # åœ¨Warmupé˜¶æ®µæ·»åŠ çº¿æ€§é¢„çƒ­
        if stage_name == 'Warmup':
            sched_config['params']['pct_start'] = 0.3  # 30%æ­¥æ•°ç”¨äºŽé¢„çƒ­
            sched_config['params']['anneal_strategy'] = 'linear'

        optimizer = OptimizerFactory.create(
            optim_type=optim_config['type'],
            model_params=self.model.parameters(),
            **optim_config['params']
        )
        criterion = CriterionFactory.create(
            criter_type=criter_config['type']
        )
        scheduler = SchedulerFactory.create(
            sched_type=sched_config['type'],
            optimizer=optimizer,
            **sched_config['params']
        )

        self.history['stages'].append({
            'name': stage_name,
            'optimizer': optimizer,
            'scheduler': scheduler,
            'criterion': criterion,
            'epochs': epochs,
            'early_stop': early_stop,
            'current_epoch': 0,
            'best_sw_score': -np.inf
        })

    def cosmic_validate(self, val_loader):
        """ éªŒè¯å‡½æ•° """
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        # æ·»åŠ éªŒè¯è¿›åº¦æ¡
        val_pbar = tqdm(total=len(val_loader), 
                        desc="\033[35mðŸŒŸ Validating\033[0m",  # ç´«è‰²
                        position=1, 
                        leave=False)

        with torch.inference_mode():
            for batch in val_loader:
                # é‡å­çº ç¼ çº§è®¾å¤‡åŒæ­¥
                iq = batch['iq'].to(self.device, non_blocking=True)
                widths = batch['sym_width'].to(self.device, non_blocking=True)
                data_len = batch['data_len'].to(self.device, non_blocking=True)

                # ç»´åº¦ç¨³å®šå‰å‘ä¼ æ’­
                with amp.autocast(device_type='cuda',
                                        # dtype=self.amp_dtype,
                                        enabled=self.scaler.is_enabled()):
                    pred = self.model(iq, data_len)
                    loss = mape_loss(pred.squeeze(), widths)

                # æ”¶é›†æ˜Ÿé™…æ•°æ®
                total_loss += loss.item()
                all_preds.extend(pred.cpu().numpy().flatten().tolist())
                all_targets.extend(widths.cpu().numpy().flatten().tolist())

                # æ›´æ–°éªŒè¯è¿›åº¦æ¡
                val_pbar.update(1)
                val_pbar.set_postfix({
                    'loss': f"\033[31m{loss.item():.4f}\033[0m"  # çº¢è‰²æ˜¾ç¤ºæŸå¤±
                })

        # è®¡ç®—å®‡å®™è¯„ä¼°æŒ‡æ ‡
        sw_score = evaluate_sw_score(all_targets, all_preds)

        val_pbar.close()

        return {
            'val_loss': total_loss / len(val_loader),
            'sw_score': sw_score
        }
    

    def cosmic_train_step(self, stage_idx: int, batch: Dict) -> float:
        """é˜¶æ®µåŒ–è®­ç»ƒæ­¥éª¤"""
        # æ·»åŠ æ··åˆç²¾åº¦ç¼“å­˜æ¸…ç†
        # torch.cuda.empty_cache()
        stage = self.history['stages'][stage_idx]
        # æ¸…ç©ºæ¢¯åº¦
        stage['optimizer'].zero_grad()

        # è®¾å¤‡è½¬ç§»ä¸Žæ··åˆç²¾åº¦
        # ä½¿ç”¨å¤šä¸ªCUDAæµå¹¶è¡Œå¤„ç†
        with torch.cuda.stream(torch.cuda.Stream()) as stream1:
            iq = batch['iq'].cuda(non_blocking=True)
            
        with torch.cuda.stream(torch.cuda.Stream()) as stream2:
            widths = batch['sym_width'].cuda(non_blocking=True)
            data_len = batch['data_len'].cuda(non_blocking=True)
    
        torch.cuda.synchronize()  # ç¡®ä¿æ•°æ®å‡†å¤‡å®Œæˆ

        # æ­£ç¡®é¡ºåºï¼šå‰å‘ä¼ æ’­ -> è®¡ç®—æŸå¤± -> åå‘ä¼ æ’­ -> å‚æ•°æ›´æ–°
        with amp.autocast(device_type='cuda'):
            pred = self.model(iq, data_len)
            if torch.isnan(pred).any():
                print("æ£€æµ‹åˆ°NaNæŸå¤±å€¼ï¼")
                raise RuntimeError("æ£€æµ‹åˆ°NaNæŸå¤±ï¼Œè®­ç»ƒå·²ç»ˆæ­¢")
            loss = mape_loss(pred.squeeze(), widths)

        # æ·»åŠ NaNå€¼æ£€æµ‹
        if torch.isnan(loss).any():
            print("æ£€æµ‹åˆ°NaNæŸå¤±å€¼ï¼")
            # loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)
            raise RuntimeError("æ£€æµ‹åˆ°NaNæŸå¤±ï¼Œè®­ç»ƒå·²ç»ˆæ­¢")

        # æ¢¯åº¦ç®¡ç†
        self.scaler.scale(loss).backward()
        # å¢žå¼ºæ¢¯åº¦è£å‰ª
        self.scaler.unscale_(stage['optimizer'])
        nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.scaler.step(stage['optimizer'])
        self.scaler.update()

        # æ›´æ–°å­¦ä¹ çŽ‡
        if isinstance(stage['scheduler'], OneCycleLR):
            stage['scheduler'].step()

        return loss.item()

    @staticmethod
    def plot_loss_curve(train_loss, val_loss, save_path):
        """
        ç”¨äºŽç»˜åˆ¶å’Œä¿å­˜æŸå¤±æ›²çº¿

        Args:
            train_loss: è®­ç»ƒæŸå¤±å€¼åˆ—è¡¨
            val_loss: éªŒè¯æŸå¤±å€¼åˆ—è¡¨
            save_path: LOSSå›¾çš„ä¿å­˜è·¯å¾„.
        """
        plt.figure(figsize=(10, 6))
        plt.plot(train_loss, label='Training Loss', color='blue', linewidth=2)
        plt.plot(val_loss, label='Validation Loss', color='red', linewidth=2)
        plt.title('Training and Validation Loss Curve')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        plt.savefig(save_path)
        plt.close()

    def launch(self, train_loader, val_loader, stages_config: list, save_model_path: str):
        """èµ›åšçŒ«å¨˜å¯åŠ¨ï¼"""
        # é¢„å…ˆä¸€æ¬¡æ€§é…ç½®æ‰€æœ‰é˜¶æ®µ
        for stage_cfg in stages_config:
            self.configure_stage(**stage_cfg)

        self.model.train()

        best_sw = -np.inf

        total_epochs = sum(s['epochs'] for s in stages_config)
        outer_pbar = tqdm(total=total_epochs, 
                     desc="\033[34mâ­ Overall Training Progress\033[0m",  # è“è‰²
                     position=0)
        
        all_train_losses = []
        all_val_losses = []

        for stage_idx, stage_cfg in enumerate(stages_config):
            # ç›´æŽ¥ä½¿ç”¨å·²é…ç½®å¥½çš„é˜¶æ®µ
            stage = self.history['stages'][stage_idx]
            stage_epochs = stage_cfg['epochs']
            early_stop = stage_cfg.get('early_stop', 10)

            no_improve = 0
            # tqdm.write(f"Current stage: {stage['name']}")
            tqdm.write(f"Initial lr: {stage['optimizer'].param_groups[0]['lr']:.2e}")
            for epoch in range(stage_epochs):
                inner_pbar = tqdm(total=len(train_loader), 
                            desc=f"\033[32mðŸš€ Stage {stage['name']} "
                                 f"Epoch {epoch + 1}/{stage_epochs}\033[0m",  # ç»¿è‰²
                            position=1, 
                            leave=False)
                # è®­ç»ƒé˜¶æ®µ
                self.model.train()
                epoch_loss = 0.0
                for train_batch in train_loader:
                    loss = self.cosmic_train_step(stage_idx, train_batch)
                    epoch_loss += loss
                    inner_pbar.update(1)
                    # æ›´æ–°å†…å±‚è¿›åº¦æ¡ä¿¡æ¯
                    inner_pbar.set_postfix({
                        'loss': f"\033[31m{loss:.4f}\033[0m",  # çº¢è‰²
                        'lr': f"\033[33m{stage['optimizer'].param_groups[0]['lr']:.2e}\033[0m"  # é»„è‰²
                    })

                inner_pbar.close()

                train_avg_loss = epoch_loss / len(train_loader)
                self.history['train_loss'].append(train_avg_loss)
                all_train_losses.append(train_avg_loss)

                # éªŒè¯é˜¶æ®µ
                self.model.eval()
                val_metrics = self.cosmic_validate(val_loader)
                self.history['val_loss'].append(val_metrics['val_loss'])
                self.history['sw_score'].append(val_metrics['sw_score'])
                all_val_losses.append(val_metrics['val_loss'])

                outer_pbar.update(1)
                outer_pbar.set_postfix({
                    'Val_loss': f"\033[31m{val_metrics['val_loss']:.4f}\033[0m",  # çº¢è‰²
                    'SW': f"\033[36m{val_metrics['sw_score']:.4f}\033[0m"  # é’è‰²
                })

                # æ›´æ–°å­¦ä¹ çŽ‡ (éžOneCycle)
                if not isinstance(stage['scheduler'], OneCycleLR):
                    if isinstance(stage['scheduler'], ReduceLROnPlateau):
                        stage['scheduler'].step(val_metrics['val_loss'])
                    else:
                        stage['scheduler'].step()

                # æ—©åœä¸Žä¿å­˜
                if val_metrics['sw_score'] > best_sw:
                    best_sw = val_metrics['sw_score']
                    # ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€
                    torch.save({
                        'model_state': self.model.state_dict(),
                        'model_arch': self.model.__class__.__name__,
                        'scaler_state': self.scaler.state_dict(),
                        'dataset_stats': {
                            'mean': self.dataset_manager.mean,
                            'std': self.dataset_manager.std
                        }
                    }, save_model_path)
                    tqdm.write(f'\033[95mà¸…(=âœ§Ï‰âœ§=) Best score {best_sw:.4f} '
                          f'at epoch {epoch + 1}\033[0m')  # ç²‰è‰²
                    no_improve = 0
                else:
                    no_improve += 1
                    if no_improve >= early_stop:
                        remaining_epochs = stage_epochs - (epoch + 1)
                        outer_pbar.update(remaining_epochs)  # æå‰æ›´æ–°å‰©ä½™çš„è¿›åº¦
                        tqdm.write(f'\033[95mðŸš¨ Stage {stage_idx + 1} early stopping at epoch {epoch + 1}\033[0m')
                        break

                # æ›´æ–°è¿›åº¦æ¡
                
                # tqdm.write(
                #     f"Stage {stage_idx + 1} | "
                #     f"LR: {stage['optimizer'].param_groups[0]['lr']:.2e} | "
                #     f"Valid loss: {val_metrics['val_loss']:.4f} | "
                #     f"Valid SW_Score: {val_metrics['sw_score']:.4f}"
                # )

            # ç»˜åˆ¶LOSSæ›²çº¿å›¾
            self.plot_loss_curve(all_train_losses, all_val_losses, loss_curve_path)

        outer_pbar.close()
        print(f'à¸…(>Ï‰<à¸…) Best Overall SW Score: {best_sw:.4f}')


class CosmicTester:
    def __init__(self,
                 model: nn.Module,
                 device: torch.device,
                 dataset_manager: DatasetManager):
        """
        é‡å­çŒ«å¨˜ç‰¹åˆ¶æµ‹è¯•å™¨

        Args:
            model: è¦è®­ç»ƒçš„æ¨¡åž‹
            device: è®­ç»ƒè®¾å¤‡
            dataset_manager: æ•°æ®é›†ç®¡ç†å¼•å…¥
        """
        self.model = model.to(device)
        self.device = device
        self.dataset_manager = dataset_manager

    def quantum_test(self, test_loader: DataLoader):
        """ å¯åŠ¨æ˜Ÿé™…æµ‹è¯•åè®® """
        self.model.eval()
        all_preds = []
        all_targets = []
        # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœèµ„æº
        with torch.no_grad():
            for batch in tqdm(test_loader, desc='ï½ž(à¹‘>á´—<à¹‘) Testing'):
                iq = batch['iq'].to(self.device)
                data_len = batch['data_len'].to(self.device)
                widths = batch['sym_width'].numpy()

                outputs = self.model(iq, data_len)
                preds = outputs.cpu().numpy()

                all_preds.extend(preds.flatten())
                all_targets.extend(widths.flatten())

        # ç”Ÿæˆå…¨æ¯æŠ¥å‘Š
        self._generate_report(all_targets, all_preds)

    @staticmethod
    def _generate_report(targets, preds):
        """ ç”Ÿæˆé‡å­æµ‹è¯•æŠ¥å‘Š """
        metrics = {
            'MAE': mean_absolute_error(targets, preds),
            'MSE': mean_squared_error(targets, preds),
            'RÂ²': r2_score(targets, preds),
            'SW Score': evaluate_sw_score(targets, preds)
        }

        print("\nðŸ“Š é‡å­æµ‹è¯•æŠ¥å‘Š:")
        for k, v in metrics.items():
            print(f"âœ¨ {k}: {v:.4f}")

        # ç»˜åˆ¶ é¢„æµ‹vsçœŸå®žå€¼å›¾è¡¨
        sw_scores = sw_score_i(targets, preds)
        plot_pred_vs_actual(targets, preds, sw_scores, pre_title='')
